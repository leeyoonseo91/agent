async function toOllamaEmbedRequest(modelName, dimensions, documents, serverAddress, requestHeaders) {
  const requestPayload = {
    model: modelName,
    input: documents.map((doc) => doc.text)
  };
  const extraHeaders = requestHeaders ? typeof requestHeaders === "function" ? await requestHeaders({
    serverAddress,
    model: {
      name: modelName,
      dimensions
    },
    embedRequest: requestPayload
  }) : requestHeaders : {};
  const headers = {
    "Content-Type": "application/json",
    ...extraHeaders
    // Add any dynamic headers
  };
  return {
    url: `${serverAddress}/api/embed`,
    requestPayload,
    headers
  };
}
function defineOllamaEmbedder(ai, { name, modelName, dimensions, options }) {
  return ai.defineEmbedder(
    {
      name: `ollama/${name}`,
      info: {
        label: "Ollama Embedding - " + name,
        dimensions,
        supports: {
          //  TODO: do any ollama models support other modalities?
          input: ["text"]
        }
      }
    },
    async (input, config) => {
      const serverAddress = config?.serverAddress || options.serverAddress;
      const { url, requestPayload, headers } = await toOllamaEmbedRequest(
        modelName,
        dimensions,
        input,
        serverAddress,
        options.requestHeaders
      );
      const response = await fetch(url, {
        method: "POST",
        headers,
        body: JSON.stringify(requestPayload)
      });
      if (!response.ok) {
        const errMsg = (await response.json()).error?.message || "";
        throw new Error(
          `Error fetching embedding from Ollama: ${response.statusText}. ${errMsg}`
        );
      }
      const payload = await response.json();
      const embeddings = [];
      for (const embedding of payload.embeddings) {
        embeddings.push({ embedding });
      }
      return { embeddings };
    }
  );
}
export {
  defineOllamaEmbedder
};
//# sourceMappingURL=embeddings.mjs.map